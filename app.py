# app.py â€” Newswire Scanner (PRNewswire + GlobeNewswire)
# í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ í•„í„°(ì •í™•ë§¤ì¹­ + ì˜ë¯¸ìœ ì‚¬ë„) & "5ëŒ€ ì¹´í…Œê³ ë¦¬ ì¤‘ â‰¥3ê°œ ì¶©ì¡±" ê·œì¹™ ì ìš© ë²„ì „
# - RSS ìˆ˜ì§‘
# - ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¹­(Phase2 ì „ìš© / Phase3 ì „ìš© / ê³µí†µ / íš¨ê³¼í¬ê¸° / ë¹„êµìš°ìœ„ì„±)
# - ë¶€ì •ë¬¸(negation) ë¬´íš¨í™”
# - ì œëª© ê°€ì¤‘ì¹˜
# - OpenAI ì„ë² ë”© ì‚¬ì „ê³„ì‚° + ìºì‹œ
# - ê¸°ì¡´ ê¸°ëŠ¥(ìš”ì•½/Discord/GSheet/ì¤‘ë³µ/í—¬ìŠ¤ì²´í¬) ìœ ì§€

import os, re, time, json, hashlib, requests, feedparser, datetime, threading, random, logging, socket
from zoneinfo import ZoneInfo
from urllib.parse import urlparse
from openai import OpenAI
from flask import Flask, jsonify
import yfinance as yf
import math

# -------------------- í™˜ê²½ ë³€ìˆ˜ --------------------
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
DISCORD_WEBHOOK_URL = os.environ["DISCORD_WEBHOOK_URL"]
GSHEET_KEY = os.getenv("GSHEET_KEY")
GOOGLE_SERVICE_ACCOUNT_JSON = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON")
REDIS_URL = os.getenv("REDIS_URL")  # ex) rediss://:password@host:port (ì—†ìœ¼ë©´ ë¡œì»¬ í´ë°±)
INSTANCE = socket.gethostname()

# -------------------- ì•±/ë¡œê±° --------------------
app = Flask(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
app.logger.setLevel(logging.INFO)
app.logger.info(f"ğŸ§­ instance={INSTANCE} | use_redis={'yes' if REDIS_URL else 'no'}")

# -------------------- RSS ì†ŒìŠ¤ --------------------
RSS_FEEDS = [
    "https://www.prnewswire.com/rss/news-releases-list.rss",
    "https://www.globenewswire.com/RssFeed/country/United%20States/feedTitle/GlobeNewswire%20-%20News%20from%20United%20States"
]

# -------------------- í•˜ì´ë¸Œë¦¬ë“œ í•„í„° ì„¤ì • --------------------
# ì„ë² ë”© ëª¨ë¸/ì„ê³„ê°’ ë“±
EMBEDDING_MODEL = "text-embedding-3-small"  # ì†ë„/ë¹„ìš© ìµœì 
SIM_THRESHOLD = 0.82                         # ì˜ë¯¸ ìœ ì‚¬ë„ ì„ê³„ê°’(ì¹´í…Œê³ ë¦¬ ëŒ€í‘œë¬¸ì¥ vs ê¸°ì‚¬)
TITLE_BONUS = 0.02                           # ì œëª© ê°€ì¤‘: íƒ€ì´íŠ¸í•œ ì„ê³„ê°’ ê²½ê³„ì—ì„œ ë°€ì–´ì¤Œ
MAX_ARTICLE_LEN = 6000                       # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´

# ë¶€ì •(negation) íŒ¨í„´: í•´ë‹¹ ë¬¸ì¥ì— ìˆìœ¼ë©´ ê¸ì • í‚¤ì›Œë“œ ë¬´íš¨í™”
NEGATIONS = re.compile(r"""(?ix)
    \b(did\s+not|does\s+not|do\s+not|has\s+not|have\s+not|failed\s+to|fail\s+to|no\s+evidence\s+of|
       not\s+statistically\s+significant|did\s+not\s+meet|missed|without\s+meaningful\s+improvement)\b
""")

# 5ê°œ ëŒ€ì¹´í…Œê³ ë¦¬ ì •ì˜(ì •í™• ë§¤ì¹­ìš© í‚¤ì›Œë“œ + ì„ë² ë”©ìš© ëŒ€í‘œ ë¬¸ì¥ ì„¸íŠ¸)
PHASE2_ONLY = [
    # í•µì‹¬ í‚¤ì›Œë“œ/ë™ì˜ì–´/íŒŒìƒ
    r"end[- ]of[- ]phase 2 meeting (planned|completed|requested|initiated)",
    r"end[- ]of[- ]phase 2 (discussion|meeting) (with|at)\s*fda",
    r"phase 3 (planning|preparation) (initiated|underway)",
    r"advancing to pivotal phase 3 (study|trial)",
    r"design of phase 3 trial (finalized|completed)",
    r"phase 3 protocol (under development|aligned with fda)",
    r"dose[- ]ranging (study|results)",
    r"dose[- ]optimization study (completed|conducted)",
    r"proof[- ]of[- ]concept (established|demonstrated|confirmed)",
    r"exploratory (endpoints|data) (achieved|support)",
    r"(refining|identified) target patient population",
    r"(optimization of|optimizing) dosing regimen",
    r"bridging study planned (before|prior to) phase 3",
    r"pivotal trial planning (underway|initiated)",
]

PHASE2_QUERIES = [
    "end-of-Phase 2 meeting planned with FDA",
    "advancing to a pivotal Phase 3 trial",
    "dose-ranging results inform Phase 3 design",
    "proof-of-concept established enabling late-stage development"
]

PHASE3_ONLY = [
    r"(met|achieved|reached|satisfied)\s+the\s+primary\s+endpoint",
    r"primary (objective|efficacy endpoint) (achieved|met)",
    r"co[- ]primary endpoints (achieved|met)",
    r"(achieved|met|exceeded)\s+(all\s+)?secondary endpoints",
    r"(superiority|non[- ]inferiority)\s+(over|to)\s+(placebo|standard of care|soc|comparator|existing therapies?)",
    r"pre[- ]nda meeting (planned|completed|held)",
    r"\bnda (submission|filing) (expected|initiated|completed|filed|underway)",
    r"rolling (submission|nda) (initiated|ongoing)",
    r"\bbla submission planned\b",
    r"pdufa date (announced|scheduled|set)",
    r"fda (has )?accepted the nda (for review)?",
    r"regulatory (review|submission) (underway|completed)",
    r"(approval decision expected|marketing authorization application|label expansion planned)",
    r"commercial launch preparation (underway|initiated)",
    r"phase 4 post[- ]marketing study planned"
]

PHASE3_QUERIES = [
    "met the primary endpoint in Phase 3",
    "company plans to submit an NDA",
    "FDA accepted the NDA and set a PDUFA date",
    "results will form the basis of regulatory submission"
]

COMMON = [
    # íš¨ëŠ¥
    r"statistically significant( (improvement|reduction|results))?",
    r"clinically meaningful (improvement|benefit|effect)",
    r"robust (efficacy|clinical response)",
    r"dose[- ]dependent response|dose[- ]response relationship",
    r"(improved|enhanced)\s+(primary outcomes?|response rate|survival rate|pfs|orr|os|qol)",
    r"(durable response|sustained efficacy)",
    r"(consistent efficacy|efficacy maintained)",
    # ì•ˆì „ì„±
    r"no (drug|treatment)[- ]related (serious )?adverse events",
    r"no serious safety signals|no unexpected safety concerns",
    r"well tolerated|favorable tolerability profile|manageable safety profile",
    r"safety profile consistent (with previous (trials|studies)|with prior studies|with standard of care)",
    r"low discontinuation rate due to adverse events",
    r"favorable (risk[- ]benefit|benefit[- ]risk) profile",
    # ê·œì œ/ì „ëµ
    r"(engaging|dialogue|interactions) with (regulators|fda)",
    r"regulatory (alignment|discussions|feedback) (achieved|incorporated|ongoing)",
    r"findings support potential (regulatory )?approval",
    r"data (will|to) form the basis of (an )?(nda|bla|submission)",
    r"(addresses|addressing) an unmet medical need",
    r"(first|best)[- ]in[- ]class (potential|profile)|potential to become standard of care",
]

COMMON_QUERIES = [
    "statistically significant improvement with clinically meaningful benefit",
    "well tolerated with no treatment-related serious adverse events",
    "findings support potential regulatory approval and address unmet medical need"
]

EFFECT_SIZE = [
    r"clinically meaningful (improvement|benefit|effect)",
    r"(robust|strong|pronounced)\s+efficacy",
    r"(marked|substantial)\s+improvement",
    r"significant effect size|large effect size",
    r"(durable response|sustained efficacy)",
    r"high (response rate|orr|cr|pr)",
    r"deep responses? observed|robust clinical response observed",
    r"meaningful reduction in",
    r"significant magnitude of response|substantial clinical impact",
]

EFFECT_SIZE_QUERIES = [
    "clinically meaningful improvement and robust efficacy",
    "substantial improvement with durable responses",
    "high overall response rate and deep responses observed"
]

COMPARATIVE = [
    r"superior (to|over)\s+(placebo|standard of care|soc|comparator|existing therapies?)",
    r"significantly greater efficacy (vs|than)\s+comparator",
    r"non[- ]inferior (to|versus)\s+\w+",
    r"non[- ]inferior and numerically superior",
    r"outperformed\s+\w+ (in|on)\s+(primary|secondary)\s+endpoints?",
    r"higher response rate (vs|than)\s+(control|comparator)",
    r"favorable efficacy profile compared (with|to) current therapies",
    r"greater magnitude of effect compared (to|with)\s+(baseline|soc|comparator)",
    r"superior benefit observed across endpoints|more effective than existing treatment options"
]

COMPARATIVE_QUERIES = [
    "superior efficacy compared to standard of care",
    "outperformed placebo across primary and secondary endpoints",
    "non-inferior to comparator with higher response rate"
]

# ì¹´í…Œê³ ë¦¬ ì»¨í…Œì´ë„ˆ
CATEGORIES = {
    "phase2_only": {"regex": [re.compile(p, re.I) for p in PHASE2_ONLY], "queries": PHASE2_QUERIES},
    "phase3_only": {"regex": [re.compile(p, re.I) for p in PHASE3_ONLY], "queries": PHASE3_QUERIES},
    "common":      {"regex": [re.compile(p, re.I) for p in COMMON],      "queries": COMMON_QUERIES},
    "effect_size": {"regex": [re.compile(p, re.I) for p in EFFECT_SIZE], "queries": EFFECT_SIZE_QUERIES},
    "comparative": {"regex": [re.compile(p, re.I) for p in COMPARATIVE], "queries": COMPARATIVE_QUERIES},
}

REQUIRED_MIN_CATS = 3  # â‰¥3ê°œ ì¹´í…Œê³ ë¦¬ ì¶©ì¡±í•´ì•¼ í†µê³¼

# -------------------- íŒŒì¼ ê²½ë¡œ/ì§€í„° --------------------
SEEN_FILE = "seen.json"
SECTOR_CACHE_FILE = "sector_cache.json"
EMBED_CACHE_FILE = "embed_cache.json"
JITTER_MIN, JITTER_MAX = 35, 50

# -------------------- ì„ íƒì : Google Sheets --------------------
use_sheets = bool(GSHEET_KEY and GOOGLE_SERVICE_ACCOUNT_JSON)

def _load_sa_json(raw: str):
    import base64, json, os
    if not raw:
        raise ValueError("GOOGLE_SERVICE_ACCOUNT_JSON is empty")
    try:
        obj = json.loads(raw)
        if isinstance(obj, dict): return obj
        if isinstance(obj, str): return json.loads(obj)
    except Exception:
        pass
    try:
        dec = base64.b64decode(raw).decode("utf-8")
        obj = json.loads(dec)
        if isinstance(obj, dict): return obj
    except Exception:
        pass
    if os.path.isfile(raw):
        with open(raw, "r") as f:
            return json.load(f)
    raise ValueError("GOOGLE_SERVICE_ACCOUNT_JSON must be a JSON string, base64-encoded JSON, or a file path")

if use_sheets:
    import gspread
    from google.oauth2.service_account import Credentials

    def append_sheet(row):
        creds = Credentials.from_service_account_info(
            _load_sa_json(GOOGLE_SERVICE_ACCOUNT_JSON),
            scopes=["https://www.googleapis.com/auth/spreadsheets"]
        )
        gc = gspread.authorize(creds)
        sh = gc.open_by_key(GSHEET_KEY)
        try:
            ws = sh.worksheet("news_log")
        except Exception:
            ws = sh.add_worksheet(title="news_log", rows=100, cols=12)
        ws.append_row(row, value_input_option="USER_ENTERED")
else:
    def append_sheet(row):
        return

# -------------------- ìœ í‹¸ --------------------
def load_json_set(path):
    if os.path.exists(path):
        with open(path, "r") as f: return set(json.load(f))
    return set()

def save_json_set(path, s):
    with open(path, "w") as f: json.dump(list(s), f)

def load_json_dict(path):
    if os.path.exists(path):
        with open(path, "r") as f: return json.load(f)
    return {}

def save_json_dict(path, d):
    with open(path, "w") as f: json.dump(d, f)

def hash_uid(url, title):
    return hashlib.sha256((url + "||" + title).encode("utf-8")).hexdigest()[:16]

def clean_url(u: str) -> str:
    p = urlparse(u)
    return f"{p.scheme}://{p.netloc}{p.path}"

def ny_kst_label(struct_time_or_none):
    if struct_time_or_none:
        dt_utc = datetime.datetime(*struct_time_or_none[:6], tzinfo=ZoneInfo("UTC"))
    else:
        dt_utc = datetime.datetime.utcnow().replace(tzinfo=ZoneInfo("UTC"))
    dt_ny = dt_utc.astimezone(ZoneInfo("America/New_York"))
    dt_kst = dt_utc.astimezone(ZoneInfo("Asia/Seoul"))
    return f"{dt_ny.strftime('%Y-%m-%d %H:%M:%S ET')} ({dt_kst.strftime('%Y-%m-%d %H:%M:%S KST')})"

# -------------------- í‹°ì»¤/íšŒì‚¬/ì„¹í„° --------------------
TICKER_PATTERNS = [
    r"\b(?:NASDAQ|Nasdaq|NYSE|AMEX|OTC|TSX|ASX|HKEX):\s*([A-Z]{1,5})\b",
    r"\$\b([A-Z]{1,5})\b",
    r"\(([A-Z]{1,5})\)",
    r"\[([A-Z]{1,5})\]",
    r"\bTICKER:\s*([A-Z]{1,5})\b",
    r"\b([A-Z]{2,5})\b"
]
def extract_ticker(text):
    for pat in TICKER_PATTERNS[:-1]:
        m = re.search(pat, text)
        if m: return m.group(1)
    m = re.search(TICKER_PATTERNS[-1], text)
    if m:
        cand = m.group(1)
        if cand.isalpha() and 2 <= len(cand) <= 5:
            return cand
    return None

COMPANY_PATTERNS = [
    r"^(.+?)\s*\((?:NASDAQ|Nasdaq|NYSE|AMEX|OTC|TSX|ASX|HKEX):\s*[A-Z]{1,5}\)\s",
    r"^(.+?)\s+(?:Announces|Launches|Introduces|Signs|Enters|Reports|Unveils|Debuts)\b",
    r"^(.+?)\s+(?:Wins|Receives|Secures|Partners|Collaborates|Expands)\b"
]
def extract_company(text):
    for pat in COMPANY_PATTERNS:
        m = re.search(pat, text, flags=re.I)
        if m:
            name = m.group(1).strip(" -â€“â€”|")
            if 2 <= len(name) <= 80: return name
    return None

def get_sector_with_cache(ticker, cache):
    if not ticker: return "Unknown"
    if ticker in cache and cache[ticker]: return cache[ticker]
    try:
        sector = yf.Ticker(ticker).info.get("sector") or "Unknown"
    except Exception:
        sector = "Unknown"
    cache[ticker] = sector
    return sector

# -------------------- OpenAI í´ë¼ì´ì–¸íŠ¸/ìš”ì•½ --------------------
client = OpenAI(api_key=OPENAI_API_KEY)
def summarize_ko(text):
    prompt = (
        "ë‹¤ìŒ ë³´ë„ìë£Œë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œë§Œ ìš”ì•½í•´ì¤˜. "
        "íšŒì‚¬ëª…, ì œí’ˆëª…, ê¸°ê´€ëª…ì€ ì˜ì–´ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  ë¶ˆí•„ìš”í•œ í•´ì„ì€ í•˜ì§€ ë§ˆ.\n\nTEXT:\n" + (text or "")[:2000]
    )
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.2,
        messages=[{"role":"user","content":prompt}],
    )
    return r.choices[0].message.content.strip()

# -------------------- ì„ë² ë”©(ì‚¬ì „ ê³„ì‚° + ìºì‹œ) --------------------
_embed_cache = load_json_dict(EMBED_CACHE_FILE)

def _vec(text: str):
    key = f"emb::{hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]}"
    if key in _embed_cache: return _embed_cache[key]
    r = client.embeddings.create(model=EMBEDDING_MODEL, input=[text])
    v = r.data[0].embedding
    _embed_cache[key] = v
    # ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥
    if random.random() < 0.05:
        save_json_dict(EMBED_CACHE_FILE, _embed_cache)
    return v

def _cos(u, v):
    # ë‘ ë²¡í„°ëŠ” ë™ì¼ ëª¨ë¸ ì°¨ì›ì´ë¼ê³  ê°€ì •
    num = sum(a*b for a,b in zip(u,v))
    den = math.sqrt(sum(a*a for a in u))*math.sqrt(sum(b*b for b in v))
    return num/den if den else 0.0

# ì¹´í…Œê³ ë¦¬ ëŒ€í‘œ ì¿¼ë¦¬ ì„ë² ë”© ì‚¬ì „ ê³„ì‚°
CATEGORY_QUERY_EMBEDS = {}
def _warmup_category_queries():
    for name, obj in CATEGORIES.items():
        qemb = []
        for q in obj["queries"]:
            qemb.append(_vec(q))
        CATEGORY_QUERY_EMBEDS[name] = qemb

_warmup_category_queries()

# -------------------- í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­ ë¡œì§ --------------------
def _normalize(s: str) -> str:
    s = (s or "")
    # ì„ë² ë”© ì…ë ¥ ê¸¸ì´ ì œí•œ
    return s[:MAX_ARTICLE_LEN]

def _has_negation(sentence: str) -> bool:
    return bool(NEGATIONS.search(sentence))

def _exact_match_any(regex_list, text):
    # ë¶€ì •ë¬¸ ë‹¨ìœ„ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë¬¸ì¥ë³„ ê²€ì‚¬
    # ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„í• (ë§ˆì¹¨í‘œ/ëŠë‚Œí‘œ/ë¬¼ìŒí‘œ ê¸°ì¤€)
    sentences = re.split(r'(?<=[\.\!\?])\s+', text)
    for sent in sentences:
        if _has_negation(sent):
            continue
        for pat in regex_list:
            if pat.search(sent):
                return True
    return False

def _semantic_match(name: str, title: str, body: str) -> bool:
    # ì œëª©/ë³¸ë¬¸ ì„ë² ë”© í•œ ë²ˆì”©
    t = _normalize(title)
    b = _normalize(body)
    tvec = _vec(t) if t else None
    bvec = _vec(b) if b else None
    qvecs = CATEGORY_QUERY_EMBEDS.get(name, [])
    if not qvecs: return False

    # ì œëª© ê°€ì¤‘ì¹˜: ì œëª© ìœ ì‚¬ë„ì— TITLE_BONUS ì¶”ê°€
    def _max_sim(doc_vec):
        if not doc_vec: return 0.0
        sims = [_cos(doc_vec, qv) for qv in qvecs]
        return max(sims) if sims else 0.0

    sim_title = _max_sim(tvec)
    sim_body  = _max_sim(bvec)

    # ë¶€ì •ë¬¸ì´ ì „ì²´ í…ìŠ¤íŠ¸ì— ê°•í•˜ê²Œ ê¹”ë¦¬ë©´ ì„ê³„ ìƒí–¥
    neg_penalty = 0.0
    if NEGATIONS.search(title) or NEGATIONS.search(body):
        neg_penalty = 0.02

    # ì„ê³„ íŒë‹¨
    if sim_title + TITLE_BONUS - neg_penalty >= SIM_THRESHOLD:
        return True
    if sim_body - neg_penalty >= SIM_THRESHOLD:
        return True
    return False

def match_categories(title: str, summary: str):
    """
    ë°˜í™˜: (matched_labels: list[str], debug_scores: dict)
    - ê° ì¹´í…Œê³ ë¦¬ëŠ” 'ì •í™•ë§¤ì¹­ OR ì˜ë¯¸ë§¤ì¹­'ì´ë©´ ì¶©ì¡±
    - ë¶€ì •ë¬¸ì€ ì •í™•ë§¤ì¹­ ë‹¨ê³„ì—ì„œ ë¬´íš¨í™”
    """
    text = f"{title} {summary}".strip()
    matched = []
    dbg = {}
    for name, obj in CATEGORIES.items():
        exact_ok = _exact_match_any(obj["regex"], text)
        sem_ok   = _semantic_match(name, title, summary)
        ok = exact_ok or sem_ok
        dbg[name] = {"exact": exact_ok, "semantic": sem_ok}
        if ok:
            matched.append(name)
    return matched, dbg

# -------------------- Discord --------------------
def discord_embed(category_labels, title, url, ticker, company, sector, article_time, summary_ko, dbg=None):
    cat_line = " | ".join(category_labels) if category_labels else "unclassified"
    desc = f"{summary_ko}\n\n"
    if dbg:
        try:
            # ê°„ë‹¨ ë””ë²„ê·¸(ì„ íƒ): ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë§¤ì¹­ëëŠ”ì§€ í‘œì‹œ
            bits = []
            for k,v in dbg.items():
                bits.append(f"{k}:E={'1' if v['exact'] else '0'}/S={'1' if v['semantic'] else '0'}")
            desc += "ğŸ§  match: " + ", ".join(bits) + "\n\n"
        except Exception:
            pass
    desc += f"ğŸ”— ì›ë¬¸: {url}"
    return {
        "username": "Newswire Scanner",
        "embeds": [{
            "title": f"[{cat_line}] {title}",
            "url": url,
            "description": desc,
            "fields": [
                {"name":"Company","value": company or "N/A", "inline": True},
                {"name":"Ticker","value": ticker or "N/A", "inline": True},
                {"name":"Sector","value": sector, "inline": True},
                {"name":"Article Time","value": article_time, "inline": False},
            ],
            "footer": {"text":"Source: PRNewswire / GlobeNewswire"}
        }]
    }

def push_discord(payload):
    try:
        resp = requests.post(DISCORD_WEBHOOK_URL, json=payload, timeout=10)
        if resp.status_code >= 300:
            app.logger.error(f"[discord] push status {resp.status_code}: {resp.text[:300]}")
    except Exception as e:
        app.logger.error(f"[discord] push error: {e}")

# -------------------- Dedup Store (Redis or local) --------------------
try:
    import redis
except Exception:
    redis = None

_rds = None
def _get_redis():
    global _rds
    if _rds is None and REDIS_URL and redis:
        try:
            _rds = redis.from_url(
                REDIS_URL,
                decode_responses=True,
                ssl=True if REDIS_URL.startswith("rediss://") else None
            )
            app.logger.info("[dedup] Redis connected")
        except Exception as e:
            app.logger.error(f"[dedup] Redis connect failed: {e}")
            _rds = None
    return _rds

def _normalize_title(title: str) -> str:
    t = (title or "").lower()
    t = re.sub(r"[\W_]+", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def seen_check_and_set_url(guid: str, ttl_seconds: int = 14*24*3600) -> bool:
    r = _get_redis()
    if r:
        key = f"nwscanner:seen:url:{guid}"
        try:
            ok = r.setnx(key, INSTANCE)
            if ok: r.expire(key, ttl_seconds)
            return not ok
        except Exception as e:
            app.logger.error(f"[dedup] Redis error(url): {e}")
    seen = load_json_set(SEEN_FILE)
    if guid in seen: return True
    seen.add(guid); save_json_set(SEEN_FILE, seen)
    return False

def seen_set_title_once(title: str, ttl_seconds: int = 14*24*3600) -> bool:
    norm = _normalize_title(title)
    if not norm: return False
    r = _get_redis()
    if r:
        key = f"nwscanner:seen:title:{hashlib.sha256(norm.encode()).hexdigest()[:16]}"
        try:
            ok = r.setnx(key, INSTANCE)
            if ok: r.expire(key, ttl_seconds)
            return not ok
        except Exception as e:
            app.logger.error(f"[dedup] Redis error(title): {e}")
            return False
    return False

# -------------------- ìŠ¤ìº” 1íšŒ --------------------
def run_once():
    app.logger.info("ğŸ” run_once: start")
    sector_cache = load_json_dict(SECTOR_CACHE_FILE)
    now_kst = datetime.datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S")

    for feed_url in RSS_FEEDS:
        app.logger.info(f"ğŸ“° fetching feed: {feed_url}")
        feed = feedparser.parse(feed_url)
        app.logger.info(f"ğŸ“° entries: {len(feed.entries)}")

        for e in feed.entries[:80]:
            url = getattr(e, "link", "") or ""
            if not url:
                app.logger.info("âš ï¸  entry skipped: no URL")
                continue

            clean = clean_url(url)
            guid = hashlib.sha256(clean.encode()).hexdigest()[:16]

            # 1) URL ê¸°ë°˜ ì¤‘ë³µ ì°¨ë‹¨
            if seen_check_and_set_url(guid):
                app.logger.info(f"â© already seen(url): {guid}")
                continue

            title = getattr(e, "title", "") or ""
            summary = getattr(e, "summary", "") or ""
            hay = f"{title} {summary}"

            # 2) í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­ + "â‰¥3 ì¹´í…Œê³ ë¦¬" ê·œì¹™
            matched_labels, dbg = match_categories(title, summary)
            if len(matched_labels) < REQUIRED_MIN_CATS:
                app.logger.info(f"âŒ <{REQUIRED_MIN_CATS} categories: {title[:100]}...")
                continue

            # 3) ì œëª© ê¸°ë°˜ ë³´ì¡° ì¤‘ë³µ ì°¨ë‹¨
            if seen_set_title_once(title):
                app.logger.info(f"â© already seen(title): {title[:80]}â€¦")
                continue

            app.logger.info(f"âœ… MATCH({len(matched_labels)} cats): {title[:120]}... â€” {matched_labels}")

            article_time = ny_kst_label(
                getattr(e, "published_parsed", None) or getattr(e, "updated_parsed", None)
            )
            ticker = extract_ticker(hay)
            company = extract_company(title) or extract_company(summary)
            sector = get_sector_with_cache(ticker, sector_cache)

            try:
                ko = summarize_ko(summary if summary else title)
            except Exception as ex:
                ko = "(ìš”ì•½ ì‹¤íŒ¨) " + (summary[:200] or title)
                app.logger.error(f"[gpt] error: {ex}")

            payload = discord_embed(matched_labels, title, url, ticker, company, sector, article_time, ko, dbg)
            push_discord(payload)
            app.logger.info("ğŸ“¤ pushed to Discord")

            # Google Sheets ê¸°ë¡
            row = [
                now_kst, feed_url.split('/')[2], guid,
                ticker or "", company or "", sector, "|".join(matched_labels),
                title, article_time, ko, url
            ]
            try:
                append_sheet(row)
                app.logger.info("[sheets] append ok")
            except Exception as e:
                app.logger.error(f"[sheets] append failed: {e}")

    save_json_dict(EMBED_CACHE_FILE, _embed_cache)
    save_json_dict(SECTOR_CACHE_FILE, sector_cache)
    app.logger.info("ğŸ” run_once: done")

# -------------------- ë°±ê·¸ë¼ìš´ë“œ ë£¨í”„ --------------------
stop_event = threading.Event()
def scanner_loop():
    app.logger.info("ğŸš€ scanner_loop: started")
    while not stop_event.is_set():
        try:
            run_once()
        except Exception as e:
            app.logger.error(f"[scanner] error: {e}")
        delay = random.uniform(JITTER_MIN, JITTER_MAX)
        app.logger.info(f"â±ï¸ sleeping {delay:.1f}s")
        time.sleep(delay)

# -------------------- Flask/Healthz --------------------
start_time = time.time()

@app.route("/")
def root():
    return "OK", 200

@app.route("/healthz")
def healthz():
    return jsonify({"status":"ok","uptime_sec": int(time.time()-start_time)}), 200

@app.route("/_sheet_test")
def sheet_test():
    try:
        now = datetime.datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S")
        append_sheet(["__TEST__", now, "write-from-app", "", "", "", "", "", "", "", ""])
        return "sheet append ok", 200
    except Exception as e:
        app.logger.error(f"[sheets] manual test failed: {e}")
        return f"sheet append failed: {e}", 500

def main():
    t = threading.Thread(target=scanner_loop, daemon=True)
    t.start()
    port = int(os.getenv("PORT", "8000"))
    app.logger.info(f"ğŸŒ starting Flask on 0.0.0.0:{port}")
    app.run(host="0.0.0.0", port=port)

if __name__ == "__main__":
    main()








